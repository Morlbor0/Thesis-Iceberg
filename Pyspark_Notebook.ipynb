{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/francescogalli/Desktop/Iceberg_Thesis_Work/.venv/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/francescogalli/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/francescogalli/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-78a1ddb7-66ff-47a9-8d22-1f577160163b;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.8.0 in central\n",
      ":: resolution report :: resolve 84ms :: artifacts dl 1ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.8.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-78a1ddb7-66ff-47a9-8d22-1f577160163b\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/3ms)\n",
      "25/03/26 15:13:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark session with Iceberg configurations, remember to start docker container ('docker compose up -d', using environment '.venv')\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark session with Iceberg configurations\n",
    "spark = SparkSession.builder \\\n",
    "  .appName(\"IcebergLocalDevelopment\") \\\n",
    "  .config('spark.jars.packages', 'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.8.0') \\\n",
    "  .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "  .config(\"spark.sql.catalog.iceberg\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "  .config(\"spark.sql.catalog.iceberg.type\", \"hadoop\") \\\n",
    "  .config(\"spark.sql.catalog.iceberg.warehouse\", \"spark-warehouse/iceberg\") \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|  namespace|\n",
      "+-----------+\n",
      "|employee_db|\n",
      "| iceberg_db|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 15:13:26 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "# See databases in Iceberg catalog\n",
    "\n",
    "spark.sql(\"SHOW DATABASES IN iceberg\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure we are referring to the correct database without having to write it down every time\n",
    "\n",
    "spark.sql(\"USE iceberg.iceberg_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+-----------+\n",
      "| namespace|        tableName|isTemporary|\n",
      "+----------+-----------------+-----------+\n",
      "|iceberg_db|           table2|      false|\n",
      "|iceberg_db|             test|      false|\n",
      "|iceberg_db|           table3|      false|\n",
      "|iceberg_db|            table|      false|\n",
      "|iceberg_db|changelog_testing|      false|\n",
      "+----------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See what tables are in the database we are using\n",
    "\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| namespace|\n",
      "+----------+\n",
      "|iceberg_db|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a database (redundant)\n",
    "\n",
    "spark.sql(\"\"\" CREATE DATABASE IF NOT EXISTS iceberg_db \"\"\")\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+\n",
      "| namespace|tableName|isTemporary|\n",
      "+----------+---------+-----------+\n",
      "|iceberg_db|   table2|      false|\n",
      "|iceberg_db|    table|      false|\n",
      "+----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a table\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          CREATE TABLE IF NOT EXISTS table (\n",
    "          id INT,\n",
    "          name STRING,\n",
    "          added_at TIMESTAMP)\n",
    "          USING iceberg\n",
    "          PARTITIONED BY (day(added_at))\"\"\")\n",
    "\n",
    "spark.sql(\"SHOW TABLES\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+\n",
      "| namespace|tableName|isTemporary|\n",
      "+----------+---------+-----------+\n",
      "|iceberg_db|   table2|      false|\n",
      "|iceberg_db|    table|      false|\n",
      "+----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a table\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          CREATE TABLE IF NOT EXISTS table2 (\n",
    "          id INT,\n",
    "          name STRING)\n",
    "          USING iceberg\n",
    "          PARTITIONED BY (id)\"\"\")\n",
    "\n",
    "spark.sql(\"SHOW TABLES\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nspark.sql(\"\"\"\\n          INSERT INTO iceberg.iceberg_db.table2 \\n          VALUES \\n          (1, \\'Alice\\'), \\n          (2, \\'Bob\\'), \\n          (3, \\'Charlie\\')\"\"\")\\n\\nspark.sql(\"SELECT * FROM iceberg.iceberg_db.table2\").show()\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Append data into table2\n",
    "\n",
    "'''\n",
    "spark.sql(\"\"\"\n",
    "          INSERT INTO iceberg.iceberg_db.table2 \n",
    "          VALUES \n",
    "          (1, 'Alice'), \n",
    "          (2, 'Bob'), \n",
    "          (3, 'Charlie')\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM iceberg.iceberg_db.table2\").show()\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+\n",
      "| id|   name|            added_at|\n",
      "+---+-------+--------------------+\n",
      "|  1|  Alice|2025-02-23 19:18:...|\n",
      "|  1|  Alice|2025-02-25 18:05:...|\n",
      "|  2|    Bob|2025-02-25 18:05:...|\n",
      "|  2|    Bob|2025-02-23 19:18:...|\n",
      "|  3|Charlie|2025-02-25 18:05:...|\n",
      "|  3|Charlie|2025-02-23 19:18:...|\n",
      "+---+-------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# See data in table2\n",
    "\n",
    "spark.sql(\"SELECT * FROM table2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+\n",
      "| id|   name|            added_at|\n",
      "+---+-------+--------------------+\n",
      "|  1|  Alice|2025-02-25 18:05:...|\n",
      "|  2|    Bob|2025-02-25 18:05:...|\n",
      "|  3|Charlie|2025-02-25 18:05:...|\n",
      "|  1|  Alice|2025-02-23 19:18:...|\n",
      "|  2|    Bob|2025-02-23 19:18:...|\n",
      "|  3|Charlie|2025-02-23 19:18:...|\n",
      "+---+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Append data into table2\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "            INSERT INTO table2 \n",
    "            VALUES\n",
    "            (1, 'Alice', current_timestamp()),\n",
    "            (2, 'Bob', current_timestamp()),\n",
    "            (3, 'Charlie', current_timestamp())\n",
    "          \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"DELETE FROM table2 \n",
    "          WHERE added_at IS NULL\n",
    "           \"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM table2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+\n",
      "| id|   name|            added_at|\n",
      "+---+-------+--------------------+\n",
      "|  1|  Alice|2025-02-23 19:18:...|\n",
      "|  2|    Bob|2025-02-23 19:18:...|\n",
      "|  3|Charlie|2025-02-23 19:18:...|\n",
      "|  1|  Alice|2025-02-25 18:05:...|\n",
      "|  2|    Bob|2025-02-25 18:05:...|\n",
      "|  3|Charlie|2025-02-25 18:05:...|\n",
      "+---+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing the time travel feature\n",
    "\n",
    "# x will be the number of minutes ago you want to query the table\n",
    "x = 1\n",
    "\n",
    "# Get the timestamp x minutes ago\n",
    "timestamp_x_minutes_ago = spark.sql(f\"SELECT current_timestamp() - INTERVAL '{x}' DAYS\").collect()[0][0]\n",
    "\n",
    "# Query the table for the timestamp x minutes ago\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT * FROM table2\n",
    "    FOR SYSTEM_TIME AS OF '{timestamp_x_minutes_ago}'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This command replaces table with a new version containing only some rows from table2 \n",
    "\n",
    "spark.sql(\"\"\"\n",
    "REPLACE TABLE table\n",
    "USING iceberg\n",
    "AS \n",
    "SELECT *\n",
    "FROM table2\n",
    "WHERE DATE(added_at) = '2025-02-23'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+\n",
      "| id|   name|            added_at|\n",
      "+---+-------+--------------------+\n",
      "|  1|  Alice|2025-02-23 19:18:...|\n",
      "|  2|    Bob|2025-02-23 19:18:...|\n",
      "|  3|Charlie|2025-02-23 19:18:...|\n",
      "+---+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|      id|      int|   NULL|\n",
      "|    name|   string|   NULL|\n",
      "|added_at|timestamp|   NULL|\n",
      "+--------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 'Superficial' table description\n",
    "\n",
    "spark.sql(\"DESCRIBE TABLE table\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id          |is_current_ancestor|\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "|2025-02-23 18:51:24.064|3195098715316441258|NULL               |true               |\n",
      "|2025-02-23 18:52:03.372|5374994393316461417|3195098715316441258|true               |\n",
      "|2025-02-23 19:07:34.053|9187256101761608397|5374994393316461417|true               |\n",
      "|2025-02-23 19:07:38.157|3014219229801966893|9187256101761608397|true               |\n",
      "|2025-02-23 19:07:40.841|9153823259828413158|3014219229801966893|true               |\n",
      "|2025-02-23 19:07:54.007|3264605670712922968|9153823259828413158|true               |\n",
      "|2025-02-23 19:18:46.366|293882809901543937 |3264605670712922968|true               |\n",
      "|2025-02-23 19:20:23.355|4306225710289109897|293882809901543937 |true               |\n",
      "|2025-02-25 18:05:57.074|3285014568135250506|4306225710289109897|true               |\n",
      "|2025-02-25 18:05:57.199|7660619263713434370|3285014568135250506|true               |\n",
      "|2025-02-27 11:34:07.001|4306425163010503355|7660619263713434370|true               |\n",
      "+-----------------------+-------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See the history of the table\n",
    "\n",
    "df = spark.read.format(\"iceberg\").load(\"iceberg_db.table2.history\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+-------------------+---------+-----------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|committed_at           |snapshot_id        |parent_id          |operation|manifest_list                                                                                                          |summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "+-----------------------+-------------------+-------------------+---------+-----------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2025-02-23 18:51:24.064|3195098715316441258|NULL               |append   |spark-warehouse/iceberg/iceberg_db/table2/metadata/snap-3195098715316441258-1-435020b2-3f3d-4988-a3a7-7c9c1374e3aa.avro|{spark.app.id -> local-1740331976643, added-data-files -> 3, added-records -> 3, added-files-size -> 1992, changed-partition-count -> 3, total-records -> 3, total-files-size -> 1992, total-data-files -> 3, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.5.4, app-id -> local-1740331976643, engine-name -> spark, iceberg-version -> Apache Iceberg 1.8.0 (commit c277c2014a1b37fe755cfe37f173b6465bb8cb73)}      |\n",
      "|2025-02-23 18:52:03.372|5374994393316461417|3195098715316441258|append   |spark-warehouse/iceberg/iceberg_db/table2/metadata/snap-5374994393316461417-1-186322ab-1e0b-45f7-bbac-511ca41d73b7.avro|{spark.app.id -> local-1740331976643, added-data-files -> 3, added-records -> 3, added-files-size -> 1992, changed-partition-count -> 3, total-records -> 6, total-files-size -> 3984, total-data-files -> 6, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.5.4, app-id -> local-1740331976643, engine-name -> spark, iceberg-version -> Apache Iceberg 1.8.0 (commit c277c2014a1b37fe755cfe37f173b6465bb8cb73)}      |\n",
      "|2025-02-23 19:07:34.053|9187256101761608397|5374994393316461417|delete   |spark-warehouse/iceberg/iceberg_db/table2/metadata/snap-9187256101761608397-1-6fa9bfaf-f852-421e-9549-95b00b1c569c.avro|{spark.app.id -> local-1740333264300, deleted-data-files -> 2, deleted-records -> 2, removed-files-size -> 1328, changed-partition-count -> 1, total-records -> 4, total-files-size -> 2656, total-data-files -> 4, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.5.4, app-id -> local-1740333264300, engine-name -> spark, iceberg-version -> Apache Iceberg 1.8.0 (commit c277c2014a1b37fe755cfe37f173b6465bb8cb73)}|\n",
      "|2025-02-23 19:07:38.157|3014219229801966893|9187256101761608397|delete   |spark-warehouse/iceberg/iceberg_db/table2/metadata/snap-3014219229801966893-1-232465cf-2acf-40de-9206-2640431a4acf.avro|{spark.app.id -> local-1740333264300, deleted-data-files -> 2, deleted-records -> 2, removed-files-size -> 1300, changed-partition-count -> 1, total-records -> 2, total-files-size -> 1356, total-data-files -> 2, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.5.4, app-id -> local-1740333264300, engine-name -> spark, iceberg-version -> Apache Iceberg 1.8.0 (commit c277c2014a1b37fe755cfe37f173b6465bb8cb73)}|\n",
      "|2025-02-23 19:07:40.841|9153823259828413158|3014219229801966893|delete   |spark-warehouse/iceberg/iceberg_db/table2/metadata/snap-9153823259828413158-1-f9bf69df-567b-4f5e-a824-8cb78a8dd8bd.avro|{spark.app.id -> local-1740333264300, deleted-data-files -> 2, deleted-records -> 2, removed-files-size -> 1356, changed-partition-count -> 1, total-records -> 0, total-files-size -> 0, total-data-files -> 0, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.5.4, app-id -> local-1740333264300, engine-name -> spark, iceberg-version -> Apache Iceberg 1.8.0 (commit c277c2014a1b37fe755cfe37f173b6465bb8cb73)}   |\n",
      "|2025-02-23 19:07:54.007|3264605670712922968|9153823259828413158|append   |spark-warehouse/iceberg/iceberg_db/table2/metadata/snap-3264605670712922968-1-c4c4bd66-28d9-437a-b15e-85458c3ea739.avro|{spark.app.id -> local-1740333264300, added-data-files -> 3, added-records -> 3, added-files-size -> 1992, changed-partition-count -> 3, total-records -> 3, total-files-size -> 1992, total-data-files -> 3, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.5.4, app-id -> local-1740333264300, engine-name -> spark, iceberg-version -> Apache Iceberg 1.8.0 (commit c277c2014a1b37fe755cfe37f173b6465bb8cb73)}      |\n",
      "|2025-02-23 19:18:46.366|293882809901543937 |3264605670712922968|append   |spark-warehouse/iceberg/iceberg_db/table2/metadata/snap-293882809901543937-1-922cce7b-9129-4579-a693-da58effc634f.avro |{spark.app.id -> local-1740333264300, added-data-files -> 3, added-records -> 3, added-files-size -> 2865, changed-partition-count -> 3, total-records -> 6, total-files-size -> 4857, total-data-files -> 6, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.5.4, app-id -> local-1740333264300, engine-name -> spark, iceberg-version -> Apache Iceberg 1.8.0 (commit c277c2014a1b37fe755cfe37f173b6465bb8cb73)}      |\n",
      "|2025-02-23 19:20:23.355|4306225710289109897|293882809901543937 |delete   |spark-warehouse/iceberg/iceberg_db/table2/metadata/snap-4306225710289109897-1-5608e580-61dd-436e-b10d-3467faaf3cdc.avro|{spark.app.id -> local-1740333264300, deleted-data-files -> 3, deleted-records -> 3, removed-files-size -> 1992, changed-partition-count -> 3, total-records -> 3, total-files-size -> 2865, total-data-files -> 3, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.5.4, app-id -> local-1740333264300, engine-name -> spark, iceberg-version -> Apache Iceberg 1.8.0 (commit c277c2014a1b37fe755cfe37f173b6465bb8cb73)}|\n",
      "|2025-02-25 18:05:57.074|3285014568135250506|4306225710289109897|append   |spark-warehouse/iceberg/iceberg_db/table2/metadata/snap-3285014568135250506-1-bd5748c3-0ad3-4266-bbde-6ec9fa00a1df.avro|{spark.app.id -> local-1740503090715, added-data-files -> 3, added-records -> 3, added-files-size -> 2865, changed-partition-count -> 3, total-records -> 6, total-files-size -> 5730, total-data-files -> 6, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.5.4, app-id -> local-1740503090715, engine-name -> spark, iceberg-version -> Apache Iceberg 1.8.0 (commit c277c2014a1b37fe755cfe37f173b6465bb8cb73)}      |\n",
      "|2025-02-25 18:05:57.199|7660619263713434370|3285014568135250506|delete   |spark-warehouse/iceberg/iceberg_db/table2/metadata/snap-7660619263713434370-1-ea6de75f-163d-4669-a5b9-86abeceec0df.avro|{spark.app.id -> local-1740503090715, changed-partition-count -> 0, total-records -> 6, total-files-size -> 5730, total-data-files -> 6, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.5.4, app-id -> local-1740503090715, engine-name -> spark, iceberg-version -> Apache Iceberg 1.8.0 (commit c277c2014a1b37fe755cfe37f173b6465bb8cb73)}                                                                           |\n",
      "+-----------------------+-------------------+-------------------+---------+-----------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See snapshots of the table, seems like a more detailed version of 'history'\n",
    "\n",
    "df = spark.read.format(\"iceberg\").load(\"iceberg_db.table2.snapshots\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update the table properties to perform row level changes\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE table2 SET TBLPROPERTIES (\n",
    "        'format-version'='2',\n",
    "        'write.delete.mode'='merge-on-read',\n",
    "        'write.update.mode'='merge-on-read',\n",
    "        'write.merge.mode'='merge-on-read'\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------------+\n",
      "| id|     name|            added_at|\n",
      "+---+---------+--------------------+\n",
      "|  2|      Bob|2025-02-25 18:05:...|\n",
      "|  1|Francesco|2025-02-23 19:18:...|\n",
      "|  1|Francesco|2025-02-25 18:05:...|\n",
      "|  3|  Charlie|2025-02-25 18:05:...|\n",
      "|  2|      Bob|2025-02-23 19:18:...|\n",
      "|  3|  Charlie|2025-02-23 19:18:...|\n",
      "+---+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Row level operations\n",
    "\n",
    "spark.sql('''\n",
    "          UPDATE table2\n",
    "          SET name = 'Francesco'\n",
    "          WHERE id = 1\n",
    "          ''')\n",
    "\n",
    "spark.sql(\"SELECT * FROM table2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                                                                                                                                                            |comment|\n",
      "+----------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n",
      "|id                          |int                                                                                                                                                                                                                  |NULL   |\n",
      "|name                        |string                                                                                                                                                                                                               |NULL   |\n",
      "|added_at                    |timestamp                                                                                                                                                                                                            |NULL   |\n",
      "|# Partition Information     |                                                                                                                                                                                                                     |       |\n",
      "|# col_name                  |data_type                                                                                                                                                                                                            |comment|\n",
      "|id                          |int                                                                                                                                                                                                                  |NULL   |\n",
      "|                            |                                                                                                                                                                                                                     |       |\n",
      "|# Metadata Columns          |                                                                                                                                                                                                                     |       |\n",
      "|_spec_id                    |int                                                                                                                                                                                                                  |       |\n",
      "|_partition                  |struct<id:int>                                                                                                                                                                                                       |       |\n",
      "|_file                       |string                                                                                                                                                                                                               |       |\n",
      "|_pos                        |bigint                                                                                                                                                                                                               |       |\n",
      "|_deleted                    |boolean                                                                                                                                                                                                              |       |\n",
      "|                            |                                                                                                                                                                                                                     |       |\n",
      "|# Detailed Table Information|                                                                                                                                                                                                                     |       |\n",
      "|Name                        |iceberg.iceberg_db.table2                                                                                                                                                                                            |       |\n",
      "|Type                        |MANAGED                                                                                                                                                                                                              |       |\n",
      "|Location                    |spark-warehouse/iceberg/iceberg_db/table2                                                                                                                                                                            |       |\n",
      "|Provider                    |iceberg                                                                                                                                                                                                              |       |\n",
      "|Owner                       |francescogalli                                                                                                                                                                                                       |       |\n",
      "|Table Properties            |[current-snapshot-id=4306425163010503355,format=iceberg/parquet,format-version=2,write.delete.mode=merge-on-read,write.merge.mode=merge-on-read,write.parquet.compression-codec=zstd,write.update.mode=merge-on-read]|       |\n",
      "+----------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See extended description of table properties\n",
    "\n",
    "spark.sql(\"DESCRIBE EXTENDED table2\").show(truncate=False, n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create new table to test row level operations\n",
    "\n",
    "spark.sql('''CREATE TABLE iceberg_db.table3 (\n",
    "    id BIGINT,\n",
    "    name STRING,\n",
    "    age INT\n",
    ") USING iceberg\n",
    "TBLPROPERTIES (\n",
    "    'format-version'='2',  -- Iceberg v2 is required for row-level deletes\n",
    "    'write.delete.mode'='merge-on-read', -- Enables row-level deletes\n",
    "    'write.update.mode'='merge-on-read', -- Enables row-level updates\n",
    "    'write.merge.mode'='merge-on-read'   -- Enables merge-on-read behavior\n",
    ");''').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Append some data into table3\n",
    "\n",
    "spark.sql('''INSERT INTO table3\n",
    "          VALUES\n",
    "(1, 'Alice', 30),\n",
    "(2, 'Bob', 40),\n",
    "(3, 'Charlie', 50)\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  1|  Alice| 30|\n",
      "|  2|    Bob| 40|\n",
      "|  3|Charlie| 50|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See data in table\n",
    "\n",
    "spark.sql(\"SELECT * FROM table3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+---+\n",
      "| id|   name|age|\n",
      "+---+-------+---+\n",
      "|  2|    Bob| 40|\n",
      "|  1|  Alice| 31|\n",
      "|  3|Charlie| 50|\n",
      "+---+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform row level operation in table3, update\n",
    "\n",
    "spark.sql('''\n",
    "          UPDATE table3\n",
    "          SET age = 31\n",
    "          WHERE id = 1\n",
    "          ''')\n",
    "spark.sql(\"SELECT * FROM table3\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view iceberg_db.table3.changelog cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Attempt to see table3 changelog (not working)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miceberg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miceberg_db.table3.changelog\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/pyspark_test/.venv/lib/python3.9/site-packages/pyspark/sql/readwriter.py:307\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/pyspark_test/.venv/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/pyspark_test/.venv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view iceberg_db.table3.changelog cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS."
     ]
    }
   ],
   "source": [
    "# Attempt to see table3 changelog (not working)\n",
    "\n",
    "df = spark.read.format(\"iceberg\").load(\"iceberg_db.table3.changelog\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-----------+\n",
      "| namespace|tableName|isTemporary|\n",
      "+----------+---------+-----------+\n",
      "|iceberg_db|   table2|      false|\n",
      "|iceberg_db|     test|      false|\n",
      "|iceberg_db|   table3|      false|\n",
      "|iceberg_db|    table|      false|\n",
      "+----------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a table\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          CREATE TABLE IF NOT EXISTS test (\n",
    "          id INT,\n",
    "          name STRING,\n",
    "          added_at TIMESTAMP)\n",
    "          USING iceberg\n",
    "          PARTITIONED BY (day(added_at))\"\"\")\n",
    "\n",
    "spark.sql(\"SHOW TABLES\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+\n",
      "| id|   name|            added_at|\n",
      "+---+-------+--------------------+\n",
      "|  1|  Alice|2025-03-05 14:34:...|\n",
      "|  2|    Bob|2025-03-05 14:34:...|\n",
      "|  3|Charlie|2025-03-05 14:34:...|\n",
      "+---+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Append data into test\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          INSERT INTO iceberg.iceberg_db.test\n",
    "          VALUES \n",
    "          (1, 'Alice', current_timestamp()), \n",
    "          (2, 'Bob', current_timestamp()), \n",
    "          (3, 'Charlie', current_timestamp())\"\"\"\n",
    "          )\n",
    "\n",
    "spark.sql(\"SELECT * FROM iceberg.iceberg_db.test\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\n[PARSE_SYNTAX_ERROR] Syntax error at or near 'RENAME'.(line 2, pos 10)\n\n== SQL ==\n\n          RENAME COLUMN test.new_name TO name\n----------^^^\n          \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;43m          RENAME COLUMN test.new_name TO name\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m          \u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM iceberg.iceberg_db.test\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/Desktop/pyspark_test_BACKUP/.venv/lib/python3.9/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/pyspark_test_BACKUP/.venv/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/pyspark_test_BACKUP/.venv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mParseException\u001b[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near 'RENAME'.(line 2, pos 10)\n\n== SQL ==\n\n          RENAME COLUMN test.new_name TO name\n----------^^^\n          \n"
     ]
    }
   ],
   "source": [
    "# Renaming a column \n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          ALTER TABLE test\n",
    "          RENAME COLUMN name TO new_name\n",
    "          \"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM iceberg.iceberg_db.test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+--------+----+\n",
      "| id|   name|            added_at|birthday| age|\n",
      "+---+-------+--------------------+--------+----+\n",
      "|  1|  Alice|2025-03-05 14:34:...|    NULL|NULL|\n",
      "|  2|    Bob|2025-03-05 14:34:...|    NULL|NULL|\n",
      "|  3|Charlie|2025-03-05 14:34:...|    NULL|NULL|\n",
      "+---+-------+--------------------+--------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"ALTER TABLE test ADD COLUMN age STRING\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM iceberg.iceberg_db.test\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+---------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id|is_current_ancestor|\n",
      "+-----------------------+-------------------+---------+-------------------+\n",
      "|2025-03-05 14:34:12.473|1156343851529063989|NULL     |true               |\n",
      "+-----------------------+-------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"iceberg\").load(\"iceberg_db.test.history\")\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+---------+-------------------+\n",
      "|     made_current_at|        snapshot_id|parent_id|is_current_ancestor|\n",
      "+--------------------+-------------------+---------+-------------------+\n",
      "|2025-03-05 14:34:...|1156343851529063989|     NULL|               true|\n",
      "+--------------------+-------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(f\"SELECT * FROM iceberg.iceberg_db.test.history\").show()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|  changelog_view|\n",
      "+----------------+\n",
      "|`table3_changes`|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CALL iceberg.system.create_changelog_view(\n",
    "  table => 'iceberg_db.table3'\n",
    ")\"\"\").show()\n",
    "\n",
    "#options => map('start-snapshot-id','1','end-snapshot-id', '2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2025-02-27 11:30:...|6029896344713609890|               NULL|   append|spark-warehouse/i...|{spark.app.id -> ...|\n",
      "|2025-02-27 11:51:...|3798345041391162531|6029896344713609890|overwrite|spark-warehouse/i...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM iceberg_db.table3.snapshots\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnsupportedOperationException",
     "evalue": "Delete files are currently not supported in changelog scans",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnsupportedOperationException\u001b[0m             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;43m\tSELECT * FROM table3_changes \u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m\tWHERE _change_type != \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDelete\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/pyspark_test_BACKUP/.venv/lib/python3.9/site-packages/pyspark/sql/dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    889\u001b[0m \n\u001b[1;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Desktop/pyspark_test_BACKUP/.venv/lib/python3.9/site-packages/pyspark/sql/dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    962\u001b[0m     )\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/pyspark_test_BACKUP/.venv/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/pyspark_test_BACKUP/.venv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mUnsupportedOperationException\u001b[0m: Delete files are currently not supported in changelog scans"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "\tSELECT * FROM table3_changes \n",
    "\tWHERE _change_type != 'Delete'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `table3_changes` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 34;\n'Distinct\n+- 'Project ['_change_type]\n   +- 'UnresolvedRelation [table3_changes], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT DISTINCT _change_type FROM table3_changes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/Desktop/pyspark_test_BACKUP/.venv/lib/python3.9/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/pyspark_test_BACKUP/.venv/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/pyspark_test_BACKUP/.venv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `table3_changes` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.; line 1 pos 34;\n'Distinct\n+- 'Project ['_change_type]\n   +- 'UnresolvedRelation [table3_changes], [], false\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT DISTINCT _change_type FROM table3_changes\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+\n",
      "|id_new|  name7|            added_at|\n",
      "+------+-------+--------------------+\n",
      "|     4|  David|2025-03-10 14:38:...|\n",
      "|     1|  Alice|2025-03-10 13:20:...|\n",
      "|     2|    Bob|2025-03-10 13:20:...|\n",
      "|     3|Charlie|2025-03-10 13:20:...|\n",
      "|     5|Richard|2025-03-10 14:40:...|\n",
      "+------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          CREATE TABLE IF NOT EXISTS changelog_testing (\n",
    "          id INT,\n",
    "          name STRING,\n",
    "          added_at TIMESTAMP)\n",
    "          USING iceberg\n",
    "          PARTITIONED BY (day(added_at))\"\"\")\n",
    "\n",
    "\"\"\"spark.sql('''INSERT INTO changelog_testing\n",
    "          VALUES\n",
    "(1, 'Alice', current_timestamp()),\n",
    "(2, 'Bob', current_timestamp()),\n",
    "(3, 'Charlie', current_timestamp())\n",
    "''')\"\"\"\n",
    "\n",
    "spark.sql(\"SELECT * FROM iceberg.iceberg_db.changelog_testing\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      changelog_view|\n",
      "+--------------------+\n",
      "|`changelog_testin...|\n",
      "+--------------------+\n",
      "\n",
      "+---+--------+--------------------+------------+---------------+-------------------+\n",
      "| id|new_name|            added_at|_change_type|_change_ordinal|_commit_snapshot_id|\n",
      "+---+--------+--------------------+------------+---------------+-------------------+\n",
      "|  1|   Alice|2025-03-10 13:20:...|      INSERT|              0|2510292383777452097|\n",
      "|  2|     Bob|2025-03-10 13:20:...|      INSERT|              0|2510292383777452097|\n",
      "|  3| Charlie|2025-03-10 13:20:...|      INSERT|              0|2510292383777452097|\n",
      "+---+--------+--------------------+------------+---------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Change the name of the 'name' column into 'new_name'\n",
    "\n",
    "spark.sql(\"\"\"ALTER TABLE changelog_testing RENAME COLUMN name TO new_name\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"CALL iceberg.system.create_changelog_view(\n",
    "  table => 'iceberg_db.changelog_testing'\n",
    ")\"\"\").show()\n",
    "\n",
    "spark.sql(\"SELECT * FROM changelog_testing_changes\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|      changelog_view|\n",
      "+--------------------+\n",
      "|`changelog_testin...|\n",
      "+--------------------+\n",
      "\n",
      "+---+-------+--------------------+------------+---------------+-------------------+\n",
      "| id|  name3|            added_at|_change_type|_change_ordinal|_commit_snapshot_id|\n",
      "+---+-------+--------------------+------------+---------------+-------------------+\n",
      "|  1|  Alice|2025-03-10 13:20:...|      INSERT|              0|2510292383777452097|\n",
      "|  2|    Bob|2025-03-10 13:20:...|      INSERT|              0|2510292383777452097|\n",
      "|  3|Charlie|2025-03-10 13:20:...|      INSERT|              0|2510292383777452097|\n",
      "+---+-------+--------------------+------------+---------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"ALTER TABLE changelog_testing RENAME COLUMN new_name TO name3\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"CALL iceberg.system.create_changelog_view(\n",
    "  table => 'iceberg_db.changelog_testing'\n",
    ")\"\"\").show()\n",
    "\n",
    "spark.sql(\"SELECT * FROM changelog_testing_changes\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+----------------+----------------------+\n",
      "|           timestamp|                file| latest_snapshot_id|latest_schema_id|latest_sequence_number|\n",
      "+--------------------+--------------------+-------------------+----------------+----------------------+\n",
      "|2025-03-10 13:20:...|spark-warehouse/i...|               NULL|            NULL|                  NULL|\n",
      "|2025-03-10 13:20:...|spark-warehouse/i...|2510292383777452097|               0|                     1|\n",
      "|2025-03-10 13:24:...|spark-warehouse/i...|2510292383777452097|               0|                     1|\n",
      "|2025-03-10 13:25:...|spark-warehouse/i...|2510292383777452097|               0|                     1|\n",
      "|2025-03-10 13:38:...|spark-warehouse/i...|2510292383777452097|               0|                     1|\n",
      "|2025-03-10 13:38:...|spark-warehouse/i...|2510292383777452097|               0|                     1|\n",
      "|2025-03-10 14:36:...|spark-warehouse/i...|2510292383777452097|               0|                     1|\n",
      "|2025-03-10 14:38:...|spark-warehouse/i...|5657999723248319641|               5|                     2|\n",
      "|2025-03-10 14:39:...|spark-warehouse/i...|5657999723248319641|               5|                     2|\n",
      "|2025-03-10 14:40:...|spark-warehouse/i...|3984517976872106574|               6|                     3|\n",
      "|2025-03-10 17:15:...|spark-warehouse/i...|3984517976872106574|               6|                     3|\n",
      "+--------------------+--------------------+-------------------+----------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM iceberg_db.changelog_testing.metadata_log_entries\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"ALTER TABLE changelog_testing RENAME COLUMN name6 TO name7\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+\n",
      "| id|  name7|            added_at|\n",
      "+---+-------+--------------------+\n",
      "|  5|Richard|2025-03-10 14:40:...|\n",
      "|  4|  David|2025-03-10 14:38:...|\n",
      "|  1|  Alice|2025-03-10 13:20:...|\n",
      "|  2|    Bob|2025-03-10 13:20:...|\n",
      "|  3|Charlie|2025-03-10 13:20:...|\n",
      "+---+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"INSERT INTO changelog_testing VALUES (5, 'Richard', current_timestamp())\")\n",
    "spark.sql(\"SELECT * FROM iceberg_db.changelog_testing\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+\n",
      "| id|  name7|            added_at|\n",
      "+---+-------+--------------------+\n",
      "|  4|  David|2025-03-10 14:38:...|\n",
      "|  5|Richard|2025-03-10 14:40:...|\n",
      "|  1|  Alice|2025-03-10 13:20:...|\n",
      "|  2|    Bob|2025-03-10 13:20:...|\n",
      "|  3|Charlie|2025-03-10 13:20:...|\n",
      "+---+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM iceberg_db.changelog_testing FOR SYSTEM_TIME AS OF '2025-03-10 14:41:00'\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+\n",
      "| id|  name5|            added_at|\n",
      "+---+-------+--------------------+\n",
      "|  1|  Alice|2025-03-10 13:20:...|\n",
      "|  2|    Bob|2025-03-10 13:20:...|\n",
      "|  3|Charlie|2025-03-10 13:20:...|\n",
      "+---+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM iceberg_db.changelog_testing\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "|name|  type|        snapshot_id|max_reference_age_in_ms|min_snapshots_to_keep|max_snapshot_age_in_ms|\n",
      "+----+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "|main|BRANCH|2510292383777452097|                   NULL|                 NULL|                  NULL|\n",
      "+----+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM iceberg_db.changelog_testing.refs\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+\n",
      "| id|   name|            added_at|\n",
      "+---+-------+--------------------+\n",
      "|  1|  Alice|2025-03-10 13:20:...|\n",
      "|  2|    Bob|2025-03-10 13:20:...|\n",
      "|  3|Charlie|2025-03-10 13:20:...|\n",
      "+---+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM iceberg_db.changelog_testing VERSION AS OF 2510292383777452097\").show()\n",
    "#SELECT * FROM prod.db.table.`tag_historical-snapshot`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2025-03-10 13:20:...|2510292383777452097|               NULL|   append|spark-warehouse/i...|{spark.app.id -> ...|\n",
      "|2025-03-10 14:38:...|5657999723248319641|2510292383777452097|   append|spark-warehouse/i...|{spark.app.id -> ...|\n",
      "|2025-03-10 14:40:...|3984517976872106574|5657999723248319641|   append|spark-warehouse/i...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM iceberg_db.changelog_testing.snapshots\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file': 'spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v1.metadata.json'},\n",
       " {'file': 'spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v2.metadata.json'},\n",
       " {'file': 'spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v3.metadata.json'},\n",
       " {'file': 'spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v4.metadata.json'},\n",
       " {'file': 'spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v5.metadata.json'},\n",
       " {'file': 'spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v6.metadata.json'},\n",
       " {'file': 'spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v7.metadata.json'},\n",
       " {'file': 'spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v8.metadata.json'},\n",
       " {'file': 'spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v9.metadata.json'},\n",
       " {'file': 'spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v10.metadata.json'}]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT file FROM iceberg_db.changelog_testing.metadata_log_entries\")\n",
    "data = df.toPandas().to_dict(orient=\"records\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[REQUIRES_SINGLE_PART_NAMESPACE] spark_catalog requires a single-part namespace, but got `iceberg_db`.`changelog_testing`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM iceberg_db.changelog_testing.history\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/Desktop/pyspark_test_BACKUP/.venv/lib/python3.9/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/pyspark_test_BACKUP/.venv/lib/python3.9/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/pyspark_test_BACKUP/.venv/lib/python3.9/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [REQUIRES_SINGLE_PART_NAMESPACE] spark_catalog requires a single-part namespace, but got `iceberg_db`.`changelog_testing`."
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM iceberg_db.changelog_testing.history\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_corrupt_record: string]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.json(\"spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v1.metadata.json\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_corrupt_record: string]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_files = [\n",
    "    row[\"file\"] for row in spark.sql(\"SELECT file FROM iceberg_db.changelog_testing.metadata_log_entries\").collect()\n",
    "]\n",
    "\n",
    "for file in metadata_files:\n",
    "    df = spark.read.json(f\"spark-warehouse/{file}\")\n",
    "    print(f\"Schema in {file}:\")\n",
    "    df.select(\"schemas\").show(truncate=False)\n",
    "\n",
    "\n",
    "df = spark.read.json(\"spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v1.metadata.json\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schemas in Metadata File:\n",
      "{\n",
      "    \"type\": \"struct\",\n",
      "    \"schema-id\": 0,\n",
      "    \"fields\": [\n",
      "        {\n",
      "            \"id\": 1,\n",
      "            \"name\": \"id\",\n",
      "            \"required\": false,\n",
      "            \"type\": \"int\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": 2,\n",
      "            \"name\": \"name\",\n",
      "            \"required\": false,\n",
      "            \"type\": \"string\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": 3,\n",
      "            \"name\": \"added_at\",\n",
      "            \"required\": false,\n",
      "            \"type\": \"timestamptz\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"type\": \"struct\",\n",
      "    \"schema-id\": 1,\n",
      "    \"fields\": [\n",
      "        {\n",
      "            \"id\": 1,\n",
      "            \"name\": \"id\",\n",
      "            \"required\": false,\n",
      "            \"type\": \"int\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": 2,\n",
      "            \"name\": \"new_name\",\n",
      "            \"required\": false,\n",
      "            \"type\": \"string\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": 3,\n",
      "            \"name\": \"added_at\",\n",
      "            \"required\": false,\n",
      "            \"type\": \"timestamptz\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"type\": \"struct\",\n",
      "    \"schema-id\": 2,\n",
      "    \"fields\": [\n",
      "        {\n",
      "            \"id\": 1,\n",
      "            \"name\": \"id\",\n",
      "            \"required\": false,\n",
      "            \"type\": \"int\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": 2,\n",
      "            \"name\": \"name3\",\n",
      "            \"required\": false,\n",
      "            \"type\": \"string\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": 3,\n",
      "            \"name\": \"added_at\",\n",
      "            \"required\": false,\n",
      "            \"type\": \"timestamptz\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"type\": \"struct\",\n",
      "    \"schema-id\": 3,\n",
      "    \"fields\": [\n",
      "        {\n",
      "            \"id\": 1,\n",
      "            \"name\": \"id\",\n",
      "            \"required\": false,\n",
      "            \"type\": \"int\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": 2,\n",
      "            \"name\": \"name4\",\n",
      "            \"required\": false,\n",
      "            \"type\": \"string\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": 3,\n",
      "            \"name\": \"added_at\",\n",
      "            \"required\": false,\n",
      "            \"type\": \"timestamptz\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"type\": \"struct\",\n",
      "    \"schema-id\": 4,\n",
      "    \"fields\": [\n",
      "        {\n",
      "            \"id\": 1,\n",
      "            \"name\": \"id\",\n",
      "            \"required\": false,\n",
      "            \"type\": \"int\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": 2,\n",
      "            \"name\": \"name5\",\n",
      "            \"required\": false,\n",
      "            \"type\": \"string\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": 3,\n",
      "            \"name\": \"added_at\",\n",
      "            \"required\": false,\n",
      "            \"type\": \"timestamptz\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"type\": \"struct\",\n",
      "    \"schema-id\": 5,\n",
      "    \"fields\": [\n",
      "        {\n",
      "            \"id\": 1,\n",
      "            \"name\": \"id\",\n",
      "            \"required\": false,\n",
      "            \"type\": \"int\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": 2,\n",
      "            \"name\": \"name6\",\n",
      "            \"required\": false,\n",
      "            \"type\": \"string\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": 3,\n",
      "            \"name\": \"added_at\",\n",
      "            \"required\": false,\n",
      "            \"type\": \"timestamptz\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "{\n",
      "    \"type\": \"struct\",\n",
      "    \"schema-id\": 6,\n",
      "    \"fields\": [\n",
      "        {\n",
      "            \"id\": 1,\n",
      "            \"name\": \"id\",\n",
      "            \"required\": false,\n",
      "            \"type\": \"int\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": 2,\n",
      "            \"name\": \"name7\",\n",
      "            \"required\": false,\n",
      "            \"type\": \"string\"\n",
      "        },\n",
      "        {\n",
      "            \"id\": 3,\n",
      "            \"name\": \"added_at\",\n",
      "            \"required\": false,\n",
      "            \"type\": \"timestamptz\"\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# The last version of the table will have information about ALL PAST SCHEMAS\n",
    "# The last version number will be stored in the version-hint.text file\n",
    "last_version_path = \"spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/version-hint.text\"\n",
    "with open(last_version_path, \"r\") as f:\n",
    "    last_version = f.read()\n",
    "\n",
    "metadata_path = f\"spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v{last_version}.metadata.json\"\n",
    "\n",
    "# Open and parse the metadata JSON file\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Print ALL available schema versions\n",
    "print(\"Schemas in Metadata File:\")\n",
    "for schema in metadata.get(\"schemas\", []):\n",
    "    print(json.dumps(schema, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------+\n",
      "|file                                                                           |\n",
      "+-------------------------------------------------------------------------------+\n",
      "|spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v1.metadata.json |\n",
      "|spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v2.metadata.json |\n",
      "|spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v3.metadata.json |\n",
      "|spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v4.metadata.json |\n",
      "|spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v5.metadata.json |\n",
      "|spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v6.metadata.json |\n",
      "|spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v7.metadata.json |\n",
      "|spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v8.metadata.json |\n",
      "|spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v9.metadata.json |\n",
      "|spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v10.metadata.json|\n",
      "|spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/v11.metadata.json|\n",
      "+-------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT file FROM iceberg_db.changelog_testing.metadata_log_entries\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "last_version_path = \"spark-warehouse/iceberg/iceberg_db/changelog_testing/metadata/version-hint.text\"\n",
    "with open(last_version_path, \"r\") as f:\n",
    "        last_version = f.read()\n",
    "        print(last_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+\n",
      "|id_new|  name7|            added_at|\n",
      "+------+-------+--------------------+\n",
      "|     1|  Alice|2025-03-10 13:20:...|\n",
      "|     2|    Bob|2025-03-10 13:20:...|\n",
      "|     3|Charlie|2025-03-10 13:20:...|\n",
      "|     4|  David|2025-03-10 14:38:...|\n",
      "|     5|Richard|2025-03-10 14:40:...|\n",
      "+------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM iceberg_db.changelog_testing\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'Phone' does not exist in the current table.\n",
      "Column 'Phone' has existed in the past. The current column of this table is named Phone number'.\n",
      "+--------------------+\n",
      "|        Phone number|\n",
      "+--------------------+\n",
      "|  (971)643-6089x9160|\n",
      "|+1-114-355-1841x7...|\n",
      "|          9017807728|\n",
      "|+1-607-333-9911x5...|\n",
      "|          3739847538|\n",
      "|001-314-829-5014x...|\n",
      "|       (314)591-7413|\n",
      "|               -7199|\n",
      "|   166-234-6882x7457|\n",
      "|  (389)824-3204x8287|\n",
      "|  (285)029-1604x5466|\n",
      "|   (233)811-1749x417|\n",
      "|       (831)049-2030|\n",
      "|          7788378816|\n",
      "|  990-374-0521x33156|\n",
      "|          2534420151|\n",
      "|   355-863-2311x6315|\n",
      "|   (566)667-8566x109|\n",
      "|          4735530004|\n",
      "|001-940-671-0693x345|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "column_name = input(\"Enter the name of the column you would like to retrieve: \")\n",
    "\n",
    "columns_set = set(spark.table(\"employee_db.employee\").columns)\n",
    "\n",
    "# Chec if the column exists in current schema, if it does simply retireve it\n",
    "if column_name in columns_set:\n",
    "    query = spark.sql(f\"SELECT `{column_name}` FROM employee_db.employee\")\n",
    "    query.show()\n",
    "\n",
    "# If it doesn't, we look at all past schema versions of the table to check if there is a column with this name\n",
    "else:\n",
    "    print(f\"Column '{column_name}' does not exist in the current table.\")\n",
    "\n",
    "    # Now we look for a column with this name in previous schema versions\n",
    "    last_version_path = \"spark-warehouse/iceberg/employee_db/employee/metadata/version-hint.text\"\n",
    "    with open(last_version_path, \"r\") as f:\n",
    "        last_version = f.read()\n",
    "\n",
    "    metadata_path = f\"spark-warehouse/iceberg/employee_db/employee/metadata/v{last_version}.metadata.json\"\n",
    "\n",
    "    # Load the JSON file with the metadata of the LAST VERSION OF THE TABLE, which will contain information about ALL PAST SCHEMAS\n",
    "    with open(metadata_path, \"r\") as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    column_name_versions = set()\n",
    "    schema_id = 0\n",
    "    # Instantiating the id of the column name we care about\n",
    "    id_of_interest = None\n",
    "\n",
    "    for schema in metadata.get(\"schemas\", []):\n",
    "        schema_id = max(schema_id, schema['schema-id'])\n",
    "        for field in schema.get('fields', []):\n",
    "            column_name_versions.add(field['name'])\n",
    "            if field['name'] == column_name:\n",
    "                # Remembering the id of the column the user looked for, so that we can retrieve it later\n",
    "                id_of_interest = field['id']\n",
    "\n",
    "\n",
    "    if column_name in column_name_versions:\n",
    "        last_schema_fields = metadata.get(\"schemas\")[schema_id]['fields']\n",
    "        # We are looking for the name of the column that the user looked for in the LAST SCHEMA VERSION\n",
    "        last_schema_fields_of_interest = [field for field in last_schema_fields if field['id'] == id_of_interest]\n",
    "        print(f\"Column '{column_name}' has existed in the past. The current column of this table is named {last_schema_fields_of_interest[0]['name']}'.\")\n",
    "        query = spark.sql(f\"SELECT `{last_schema_fields_of_interest[0]['name']}` FROM employee_db.employee\")\n",
    "        query.show()\n",
    "    else:\n",
    "        print(\"Could not find the column in neither the current table schema nor the schema history.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT name FROM iceberg_db.changelog_testing\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+\n",
      "|id_new|  name7|            added_at|\n",
      "+------+-------+--------------------+\n",
      "|     4|  David|2025-03-10 14:38:...|\n",
      "|     5|Richard|2025-03-10 14:40:...|\n",
      "|     1|  Alice|2025-03-10 13:20:...|\n",
      "|     2|    Bob|2025-03-10 13:20:...|\n",
      "|     3|Charlie|2025-03-10 13:20:...|\n",
      "+------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM iceberg_db.changelog_testing\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------------+\n",
      "| id|     name|            added_at|\n",
      "+---+---------+--------------------+\n",
      "|  2|      Bob|2025-02-23 19:18:...|\n",
      "|  3|  Charlie|2025-02-23 19:18:...|\n",
      "|  1|Francesco|2025-02-23 19:18:...|\n",
      "|  1|Francesco|2025-02-25 18:05:...|\n",
      "|  2|      Bob|2025-02-25 18:05:...|\n",
      "|  3|  Charlie|2025-02-25 18:05:...|\n",
      "+---+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM iceberg_db.table2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+\n",
      "|id_new|  name7|            added_at|\n",
      "+------+-------+--------------------+\n",
      "|     5|Richard|2025-03-10 14:40:...|\n",
      "|     4|  David|2025-03-10 14:38:...|\n",
      "|     1|  Alice|2025-03-10 13:20:...|\n",
      "|     2|    Bob|2025-03-10 13:20:...|\n",
      "|     3|Charlie|2025-03-10 13:20:...|\n",
      "+------+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"ALTER TABLE changelog_testing RENAME COLUMN id TO id_new\")\n",
    "spark.sql(\"SELECT * FROM iceberg_db.changelog_testing\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|changelog_view             |\n",
      "+---------------------------+\n",
      "|`changelog_testing_changes`|\n",
      "+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CALL iceberg.system.create_changelog_view(\n",
    "  table => 'iceberg_db.changelog_testing'\n",
    "  )\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+------------+---------------+-------------------+\n",
      "|id_new|  name7|            added_at|_change_type|_change_ordinal|_commit_snapshot_id|\n",
      "+------+-------+--------------------+------------+---------------+-------------------+\n",
      "|     1|  Alice|2025-03-10 13:20:...|      INSERT|              0|2510292383777452097|\n",
      "|     2|    Bob|2025-03-10 13:20:...|      INSERT|              0|2510292383777452097|\n",
      "|     3|Charlie|2025-03-10 13:20:...|      INSERT|              0|2510292383777452097|\n",
      "|     4|  David|2025-03-10 14:38:...|      INSERT|              1|5657999723248319641|\n",
      "|     5|Richard|2025-03-10 14:40:...|      INSERT|              2|3984517976872106574|\n",
      "+------+-------+--------------------+------------+---------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM changelog_testing_changes\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|          parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "|2025-03-10 13:20:...|2510292383777452097|               NULL|   append|spark-warehouse/i...|{spark.app.id -> ...|\n",
      "|2025-03-10 14:38:...|5657999723248319641|2510292383777452097|   append|spark-warehouse/i...|{spark.app.id -> ...|\n",
      "|2025-03-10 14:40:...|3984517976872106574|5657999723248319641|   append|spark-warehouse/i...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+-------------------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM iceberg_db.changelog_testing.snapshots\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+\n",
      "| id|  name6|            added_at|\n",
      "+---+-------+--------------------+\n",
      "|  1|  Alice|2025-03-10 13:20:...|\n",
      "|  2|    Bob|2025-03-10 13:20:...|\n",
      "|  3|Charlie|2025-03-10 13:20:...|\n",
      "|  4|  David|2025-03-10 14:38:...|\n",
      "+---+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM iceberg_db.changelog_testing VERSION AS OF 5657999723248319641\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+----------------+----------------------+\n",
      "|           timestamp|                file| latest_snapshot_id|latest_schema_id|latest_sequence_number|\n",
      "+--------------------+--------------------+-------------------+----------------+----------------------+\n",
      "|2025-03-10 13:20:...|spark-warehouse/i...|               NULL|            NULL|                  NULL|\n",
      "|2025-03-10 13:20:...|spark-warehouse/i...|2510292383777452097|               0|                     1|\n",
      "|2025-03-10 13:24:...|spark-warehouse/i...|2510292383777452097|               0|                     1|\n",
      "|2025-03-10 13:25:...|spark-warehouse/i...|2510292383777452097|               0|                     1|\n",
      "|2025-03-10 13:38:...|spark-warehouse/i...|2510292383777452097|               0|                     1|\n",
      "|2025-03-10 13:38:...|spark-warehouse/i...|2510292383777452097|               0|                     1|\n",
      "|2025-03-10 14:36:...|spark-warehouse/i...|2510292383777452097|               0|                     1|\n",
      "|2025-03-10 14:38:...|spark-warehouse/i...|5657999723248319641|               5|                     2|\n",
      "|2025-03-10 14:39:...|spark-warehouse/i...|5657999723248319641|               5|                     2|\n",
      "|2025-03-10 14:40:...|spark-warehouse/i...|3984517976872106574|               6|                     3|\n",
      "|2025-03-10 17:15:...|spark-warehouse/i...|3984517976872106574|               6|                     3|\n",
      "+--------------------+--------------------+-------------------+----------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM iceberg_db.changelog_testing.metadata_log_entries\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Name\n"
     ]
    }
   ],
   "source": [
    "# TESTING CODE, NOT ACTUAL CODE\n",
    "\n",
    "# The last version of the table will have information about ALL PAST SCHEMAS\n",
    "# The last version number will be stored in the version-hint.text file\n",
    "last_version_path = \"spark-warehouse/iceberg/employee_db/employee/metadata/version-hint.text\"\n",
    "with open(last_version_path, \"r\") as f:\n",
    "    last_version = f.read()\n",
    "\n",
    "metadata_path = f\"spark-warehouse/iceberg/employee_db/employee/metadata/v{last_version}.metadata.json\"\n",
    "\n",
    "# Open and parse the metadata JSON file\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "# Print all the names that the column 'name' has had across all schema versions\n",
    "column_name_versions = set()\n",
    "schema_id = 0\n",
    "id_of_interest = None\n",
    "\n",
    "for schema in metadata.get(\"schemas\", []):\n",
    "    schema_id = max(schema_id, schema['schema-id'])\n",
    "    for field in schema.get('fields', []):\n",
    "        column_name_versions.add(field['name'])\n",
    "        if field['name'] == 'name':\n",
    "                id_of_interest = field['id']\n",
    "\n",
    "column_name_versions\n",
    "# Problem: there is no recollection of WHEN THE COLUMNS WERE NAMED AS SUCH, and when the schema changed.\n",
    "# Do we care? \n",
    "\n",
    "last_schema_fields = metadata.get(\"schemas\")[schema_id]['fields']\n",
    "last_schema_fields_of_interest = [field for field in last_schema_fields if field['id'] == 2]\n",
    "print(last_schema_fields_of_interest[0]['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+------+--------------------+--------------------+-------------+--------------------+\n",
      "|Index|First Name| Last Name|   Sex|               Email|               Phone|Date of birth|           Job Title|\n",
      "+-----+----------+----------+------+--------------------+--------------------+-------------+--------------------+\n",
      "|    1|      Sara|   Mcguire|Female|  tsharp@example.net|  (971)643-6089x9160|   2021-08-17|Editor, commissio...|\n",
      "|    2|    Alisha|    Hebert|  Male|vincentgarrett@ex...|+1-114-355-1841x7...|   1969-06-28|  Broadcast engineer|\n",
      "|    3| Gwendolyn|  Sheppard|  Male|mercadojonathan@e...|          9017807728|   2015-09-25|    Industrial buyer|\n",
      "|    4|  Kristine|    Mccann|Female|lindsay55@example...|+1-607-333-9911x5...|   1978-07-27|Multimedia specia...|\n",
      "|    5|     Bobby|   Pittman|Female|blevinsmorgan@exa...|          3739847538|   1989-11-17|Planning and deve...|\n",
      "|    6|    Calvin|    Ramsey|Female|loretta85@example...|001-314-829-5014x...|   2017-08-31|Therapeutic radio...|\n",
      "|    7|    Collin|   Allison|  Male| yvaughn@example.net|       (314)591-7413|   1979-11-21|       Administrator|\n",
      "|    8|  Nicholas|    Branch|  Male|greerjimmy@exampl...|               -7199|   2006-01-21|   Fisheries officer|\n",
      "|    9|      Emma|  Robinson|Female|charleshiggins@ex...|   166-234-6882x7457|   2009-03-19|       Haematologist|\n",
      "|   10|     Pedro|   Cordova|  Male|leslie08@example.com|  (389)824-3204x8287|   2008-06-17|      Phytotherapist|\n",
      "|   11|      Jean|   Aguilar|  Male|raymond24@example...|  (285)029-1604x5466|   1978-10-28|Engineer, control...|\n",
      "|   12|    Dwayne|Hutchinson|Female|preston28@example...|   (233)811-1749x417|   2014-06-22|Teacher, early ye...|\n",
      "|   13|   Bradley| Velazquez|  Male|joycejay@example.net|       (831)049-2030|   2006-12-23|       Airline pilot|\n",
      "|   14|  Samantha|      Moon|  Male|wesleymullins@exa...|          7788378816|   1978-06-21|Intelligence analyst|\n",
      "|   15|   Clayton|  Erickson|  Male|  nterry@example.net|  990-374-0521x33156|   1959-02-07|    Paediatric nurse|\n",
      "|   16|   Bradley|    Bright|Female|devinherman@examp...|          2534420151|   1992-01-27|Fitness centre ma...|\n",
      "|   17|    Ernest|   Maynard|  Male|tommywoodard@exam...|   355-863-2311x6315|   2017-11-23|Accountant, chart...|\n",
      "|   18|       Joe|    Larsen|Female|brianna43@example...|   (566)667-8566x109|   2009-06-18|Geographical info...|\n",
      "|   19|    Alfred|   Barrera|Female|kristina40@exampl...|          4735530004|   2024-11-13|Administrator, ch...|\n",
      "|   20|       Jon|    Wagner|Female|meagan99@example.org|001-940-671-0693x345|   1934-01-06|Engineer, aeronau...|\n",
      "+-----+----------+----------+------+--------------------+--------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loading the employee 1000x dataset\n",
    "df = pd.read_csv('Employee 1000x.csv')\n",
    "\n",
    "# Converting the 'Date of birth' column to datetime format\n",
    "df['Date of birth'] = pd.to_datetime(df['Date of birth'], format='%d-%m-%y', errors='coerce').dt.date\n",
    "\n",
    "# Replacing the year of birth if it is in the future (I was getting years of birth like 2059)\n",
    "df['Date of birth'] = df['Date of birth'].apply(lambda x: x.replace(year=x.year - 100) if x and x.year > pd.Timestamp.now().year else x)\n",
    "\n",
    "# Creating a Spark DataFrame from the pandas DataFrame\n",
    "spark_df = spark.createDataFrame(df)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Index', 'bigint'),\n",
       " ('First Name', 'string'),\n",
       " ('Last Name', 'string'),\n",
       " ('Sex', 'string'),\n",
       " ('Email', 'string'),\n",
       " ('Phone', 'string'),\n",
       " ('Date of birth', 'date'),\n",
       " ('Job Title', 'string')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+-----+----------+----------+------+--------------------+--------------------+-------------+--------------------+\n",
      "|Index|First Name| Last Name|   Sex|               Email|               Phone|Date of birth|           Job Title|\n",
      "+-----+----------+----------+------+--------------------+--------------------+-------------+--------------------+\n",
      "|    1|      Sara|   Mcguire|Female|  tsharp@example.net|  (971)643-6089x9160|   2021-08-17|Editor, commissio...|\n",
      "|    2|    Alisha|    Hebert|  Male|vincentgarrett@ex...|+1-114-355-1841x7...|   1969-06-28|  Broadcast engineer|\n",
      "|    3| Gwendolyn|  Sheppard|  Male|mercadojonathan@e...|          9017807728|   2015-09-25|    Industrial buyer|\n",
      "|    4|  Kristine|    Mccann|Female|lindsay55@example...|+1-607-333-9911x5...|   1978-07-27|Multimedia specia...|\n",
      "|    5|     Bobby|   Pittman|Female|blevinsmorgan@exa...|          3739847538|   1989-11-17|Planning and deve...|\n",
      "|    6|    Calvin|    Ramsey|Female|loretta85@example...|001-314-829-5014x...|   2017-08-31|Therapeutic radio...|\n",
      "|    7|    Collin|   Allison|  Male| yvaughn@example.net|       (314)591-7413|   1979-11-21|       Administrator|\n",
      "|    8|  Nicholas|    Branch|  Male|greerjimmy@exampl...|               -7199|   2006-01-21|   Fisheries officer|\n",
      "|    9|      Emma|  Robinson|Female|charleshiggins@ex...|   166-234-6882x7457|   2009-03-19|       Haematologist|\n",
      "|   10|     Pedro|   Cordova|  Male|leslie08@example.com|  (389)824-3204x8287|   2008-06-17|      Phytotherapist|\n",
      "|   11|      Jean|   Aguilar|  Male|raymond24@example...|  (285)029-1604x5466|   1978-10-28|Engineer, control...|\n",
      "|   12|    Dwayne|Hutchinson|Female|preston28@example...|   (233)811-1749x417|   2014-06-22|Teacher, early ye...|\n",
      "|   13|   Bradley| Velazquez|  Male|joycejay@example.net|       (831)049-2030|   2006-12-23|       Airline pilot|\n",
      "|   14|  Samantha|      Moon|  Male|wesleymullins@exa...|          7788378816|   1978-06-21|Intelligence analyst|\n",
      "|   15|   Clayton|  Erickson|  Male|  nterry@example.net|  990-374-0521x33156|   1959-02-07|    Paediatric nurse|\n",
      "|   16|   Bradley|    Bright|Female|devinherman@examp...|          2534420151|   1992-01-27|Fitness centre ma...|\n",
      "|   17|    Ernest|   Maynard|  Male|tommywoodard@exam...|   355-863-2311x6315|   2017-11-23|Accountant, chart...|\n",
      "|   18|       Joe|    Larsen|Female|brianna43@example...|   (566)667-8566x109|   2009-06-18|Geographical info...|\n",
      "|   19|    Alfred|   Barrera|Female|kristina40@exampl...|          4735530004|   2024-11-13|Administrator, ch...|\n",
      "|   20|       Jon|    Wagner|Female|meagan99@example.org|001-940-671-0693x345|   1934-01-06|Engineer, aeronau...|\n",
      "+-----+----------+----------+------+--------------------+--------------------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.createOrReplaceTempView(\"Employee1000\")\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS iceberg.employee_db\")\n",
    "spark.sql(\"USE iceberg.employee_db\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "          CREATE TABLE IF NOT EXISTS\n",
    "          employee\n",
    "          USING iceberg\n",
    "          AS SELECT * FROM Employee1000\n",
    "          \"\"\").show()\n",
    "\n",
    "spark.sql(\"SELECT * FROM iceberg.employee_db.employee LIMIT 20\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------+\n",
      "|     col_name|data_type|comment|\n",
      "+-------------+---------+-------+\n",
      "|        Index|   bigint|   NULL|\n",
      "|   First Name|   string|   NULL|\n",
      "|    Last Name|   string|   NULL|\n",
      "|          Sex|   string|   NULL|\n",
      "|        Email|   string|   NULL|\n",
      "|        Phone|   string|   NULL|\n",
      "|Date of birth|     date|   NULL|\n",
      "|    Job Title|   string|   NULL|\n",
      "+-------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE employee\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+------+--------------------+--------------------+-------------+--------------------+\n",
      "|Index|First Name| Last Name|   Sex|               Email|               Phone|Date of birth|           Job Title|\n",
      "+-----+----------+----------+------+--------------------+--------------------+-------------+--------------------+\n",
      "|  128|      Glen|  Hamilton|  Male|nashantonio@examp...|+1-478-796-8374x9...|   1926-04-14|Special education...|\n",
      "|  155|     Jamie|     Riggs|  Male| pmalone@example.net|   (185)304-1686x938|   1926-01-09|Conservation offi...|\n",
      "|  161|    Hunter|    Romero|Female|austinruben@examp...|   977-555-6372x3075|   1926-08-21|Therapeutic radio...|\n",
      "|  209|   Maxwell|     Floyd|Female|fryejoanne@exampl...|          1197827255|   1926-01-24|           Osteopath|\n",
      "|  258|     Diana|    Travis|Female|bishopkayla@examp...|   771-351-1656x3614|   1926-07-12|Volunteer coordin...|\n",
      "|  426|      Sean|   Shelton|Female|mccarthyvernon@ex...|   585-660-0047x2644|   1926-05-19|Producer, televis...|\n",
      "|  607|    Sherri|    Conrad|Female| sacosta@example.net|   (546)262-2628x488|   1926-08-25|Copywriter, adver...|\n",
      "|  737|     Marco|    Suarez|Female|yeseniawilliamson...|001-986-846-4146x...|   1926-02-24|Human resources o...|\n",
      "|  745|    Curtis|    Reeves|  Male|marvin05@example.com|          6170858046|   1926-07-10| Art gallery manager|\n",
      "|  765|   Richard|   Mathews|  Male|banksluke@example...|    585-053-2252x621|   1926-04-21|Database administ...|\n",
      "|  786|    Deanna|    Vaughn|  Male|raymckee@example.com|        422.969.6683|   1926-01-13|        Cartographer|\n",
      "|  796|  Angelica|Washington|Female| wbenton@example.org|        966.783.0853|   1926-09-12|Health and safety...|\n",
      "|  811|     Wanda|    Farley|Female|benjamincombs@exa...|          1452727936|   1926-05-21|Museum/gallery ex...|\n",
      "|  891|     Danny| Dominguez|  Male|mfischer@example.net|          6914840287|   1926-05-13|  Physicist, medical|\n",
      "| 1140|    Gloria|    Suarez|  Male|galvankatherine@e...|  (574)162-2018x8589|   1926-09-15| Designer, furniture|\n",
      "| 1489|     Lance|    Gentry|Female| ggraham@example.net| +1-308-764-9859x758|   1926-01-16|  Editor, film/video|\n",
      "| 1634|   Caitlyn|     Huang|  Male| lhebert@example.com| +1-134-672-5217x151|   1926-04-11|        Tour manager|\n",
      "| 1696|      John|  Valencia|  Male|  neil75@example.org|        027.446.8157|   1926-12-04|   Probation officer|\n",
      "| 1874|    Meghan| Stevenson|  Male|martha36@example.org|  (908)499-5788x5024|   1926-07-09|Sales promotion a...|\n",
      "| 1909|     Katie|     Morse|Female|guzmanjoyce@examp...|001-331-630-4325x...|   1926-04-12|Secondary school ...|\n",
      "+-----+----------+----------+------+--------------------+--------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM employee WHERE YEAR(`Date of birth`) < 1927\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"ALTER TABLE employee_db.employee RENAME COLUMN Phone TO `Phone number`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+----------------+----------------------+\n",
      "|           timestamp|                file| latest_snapshot_id|latest_schema_id|latest_sequence_number|\n",
      "+--------------------+--------------------+-------------------+----------------+----------------------+\n",
      "|2025-03-26 15:15:...|spark-warehouse/i...|1563362177603208424|               0|                     1|\n",
      "|2025-03-26 15:28:...|spark-warehouse/i...|1563362177603208424|               0|                     1|\n",
      "+--------------------+--------------------+-------------------+----------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM employee_db.employee.metadata_log_entries\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+---------+---------+--------------------+--------------------+\n",
      "|        committed_at|        snapshot_id|parent_id|operation|       manifest_list|             summary|\n",
      "+--------------------+-------------------+---------+---------+--------------------+--------------------+\n",
      "|2025-03-26 15:15:...|1563362177603208424|     NULL|   append|spark-warehouse/i...|{spark.app.id -> ...|\n",
      "+--------------------+-------------------+---------+---------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM employee_db.employee.snapshots\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Name\n"
     ]
    }
   ],
   "source": [
    "table_name = \"employee\"\n",
    "\n",
    "last_version_path = f\"spark-warehouse/iceberg/employee_db/{table_name}/metadata/version-hint.text\"\n",
    "with open(last_version_path, \"r\") as f:\n",
    "    last_version = f.read()\n",
    "metadata_path = f\"spark-warehouse/iceberg/employee_db/{table_name}/metadata/v{last_version}.metadata.json\"\n",
    "with open(metadata_path, \"r\") as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "column_name_versions = set()\n",
    "schema_id = 0\n",
    "# Instantiating the id of the column name we care about\n",
    "id_of_interest = 2\n",
    "\n",
    "\n",
    "column_info = metadata.get('schemas')[-1].get('fields')\n",
    "column_name = column_info[id_of_interest - 1].get('name')\n",
    "print(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+----------+------+--------------------+--------------------+-------------+--------------------+\n",
      "|Index|First Name| Last Name|   Sex|               Email|        Phone number|Date of birth|           Job Title|\n",
      "+-----+----------+----------+------+--------------------+--------------------+-------------+--------------------+\n",
      "|  146|     Sarah|  Humphrey|Female|tammie97@example.org| (135)381-5257x34881|   2007-08-03|Conservator, muse...|\n",
      "|  160|     Sarah|     Huber|Female|wilkersonlucas@ex...|        893.355.7289|   2021-05-20|Chartered managem...|\n",
      "| 2436|     Sarah|  Franklin|Female|connor73@example.net|  189.043.5346x35696|   1960-04-17|           Ecologist|\n",
      "| 2464|     Sarah|    Church|  Male|   vbond@example.com|001-224-972-5823x...|   1942-10-31|  Broadcast engineer|\n",
      "| 2661|     Sarah|    Lowery|  Male|atkinscheryl@exam...|               -4781|   2002-03-20|Diplomatic Servic...|\n",
      "| 4759|     Sarah|     Nixon|  Male|candice04@example...| +1-364-324-2704x270|   1996-04-02|       Tax inspector|\n",
      "| 5073|     Sarah|  Chambers|Female|  yhines@example.org|        377-829-0002|   1931-05-17| Information officer|\n",
      "| 5299|     Sarah|     Villa|Female|joanne53@example.net|+1-762-421-4434x5...|   1996-03-17|Management consul...|\n",
      "| 5354|     Sarah|    Mooney|  Male|denisehammond@exa...|    016.903.3443x547|   1986-06-08| Medical illustrator|\n",
      "| 7375|     Sarah|   Wheeler|Female|lynchgavin@exampl...|001-173-245-8966x...|   2014-08-18|  Field seismologist|\n",
      "| 8491|     Sarah|      Mann|Female|fmcdaniel@example...|001-296-700-4121x233|   1976-03-07|Television camera...|\n",
      "| 9863|     Sarah|      Ryan|  Male|ufletcher@example...|        439.877.4049|   1963-07-22|Medical technical...|\n",
      "| 9974|     Sarah|Williamson|Female|bryankirk@example...|       (162)191-6599|   1931-08-05|Customer service ...|\n",
      "+-----+----------+----------+------+--------------------+--------------------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"SELECT * FROM iceberg.employee_db.employee WHERE `First Name` = 'Sarah'\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
